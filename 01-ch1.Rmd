---
title: "Data visulisation using R, for researchers who don't use R"
author: "Emily Nordmann, Phil McAleer, Wilhelmiina Toivo, Helena Paterson, Lisa DeBruine"
date: "11/05/2021"
bibliography: book.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  message = FALSE,
  warning = FALSE,
  out.width = "100%",
  fig.cap = "**CAPTION THIS**"
)
library(knitr)
library(tidyverse)
library(patchwork)
dat <- read_csv(file = "stroop_data.csv")
dat <- dat %>%
  mutate(language = factor(
    x = language, # column to translate
    levels = c(1, 2), # values of the original data in preferred order
    labels = c("monolingual", "bilingual") # labels for display
  ))
dat <- dat %>%
  mutate(language = factor(language, 
                           c(1, 2), 
                           c("monolingual", "bilingual")))
long <- pivot_longer(dat, 
                     cols = rt_cong:acc_incon, 
                     names_to = c("dv_type", "condition"), 
                     names_sep = "_", 
                     values_to = "dv")

dat_long <- pivot_wider(long, 
                        names_from = "dv_type", 
                        values_from = "dv")
```

# Introduction

Use of the programming language R for data processing and statistical analysis by researchers is increasingly common with a GET THE STATS FROM THAT THING I SAW ABOUT R ON TWITTER rise since XXXX (REF). In addition to benefiting reproducibility and transparency, one of the advantages of using R is that researchers have a much larger range of fully customisable data visualisations options than are typically available in point-and-click software, due to the open-source nature of R. These visualisation options not only look attractive, but can increase transparency about the distribution of the underlying data rather than relying on commonly used visualisations of aggregations such as bar charts of means (REF HERE ABOUT BAR CHARTS).

Yet, the benefits of using R are hindered by its notoriously steep learning curve (REF - is there a ref?) and that that only a minority of psychology programmes currently teach programming skills (REF, PsyTeachR ref) with the majority of both undergraduate and postgraduate courses using proprietary point-and-click software such as SPSS or Microsoft Excel.

In this paper we aim to provide a practical introduction to data visualisation using R, specifically aimed at researchers who have little to no prior experience using R. We detail the rationale for using R for data visualisation, introduce the "grammar of graphics" that underlies data visualisation using the `ggplot` package, and provide a tutorial that walks the reader through how to replicate plots that are commonly available in point-and-click software such as histograms and boxplots, as well as showing how the code for these "basic" plots can be easily extended to less commonly available options such as violin-boxplots, raincloud plots, and heat-maps.

## Why R for data visualisation?

Data visualisation benefits from the same advantages as statistical analysis when writing code rather than using point-and-click software -- reproducibility and transparency. The need for psychological researchers to work in reproducible ways has been well-documented and discussed in response to the replication crisis (REFs) and we will not repeat these arguments here. However, there is an additional selfish benefit to reproducibility that is less frequently acknowledged compared to the loftier goals of improving psychological science: if you write code to produce your plots, future-you can reuse and adapt that code rather than starting from scratch each time.

In addition to the benefits of reproducibility, using R for data visualisation gives the researcher almost total control over each element of the plot. Whilst this flexibility can seem daunting to novice users of R, if one can survive the initial learning curve, the ability to write reusable code recipes (and use recipes created by others) is highly advantageous. The level of customisation and the professional outputs available using R has led to news outlets such as the BBC [@BBC-R] and the New York Times [@NYT-R] to adopt R as their preferred data visualisation tool.

## A layered grammar of graphics

There are multiple approaches to data visualisation in R; in this paper we will be using the popular package[^1] `ggplot2` [@ggplot2] which is part of the larger `tidyverse`[^2] [@tidyverse] collection of packages that provide functions for data wrangling, descriptives, and visualisation. A grammar of graphics [@wilkinson2005graph] is a standardised way to describe the components of a graphic. `ggplot2` uses a layered grammar of graphics [@wickham2010layered], in which plots are built up in a series of layers. This approach may be familiar to users of MATLAB but can be unintuitive to those used to creating plots in Excel or SPSS.

[^1]: The power of R is that it is extendable and open source - put simply, if a function doesn't exist or is difficult to use, anyone can create a new **package** that contains data and code to allow you to perform new tasks. You may find it helpful to think of packages as additional apps that you need to download separately to extend the functionality beyond what comes with "Base R".

[^2]: Because there are so many different ways to achieve the same thing in R, when Googling for help with R, it is useful to append the name of the package or approach you are using, e.g., "how to make a histogram ggplot2".

Figure\ \@ref(fig:layers) displays the evolution of a simple scatterplot using this layered approach. First, the plot space is built (layer 1); the variables are specified (layer 2); the type of visualisation (known as a `geom`) that is desired for these variables is specified (layer 3) - in this case `geom_point()` is called to visualise individual data points; a second geom is added to include a line of best fit (layer 4), the axis labels are edited for readability (layer 5), and finally, a theme is applied to change the overall appearance of the plot (layer 6).

```{r layers, echo = FALSE, message = FALSE, fig.cap="Evolution of a layered plot"}
library(patchwork)
library(tidyverse)
a <- ggplot() + labs(subtitle = "Layer 1")
b <- ggplot(iris, aes(Sepal.Length, Sepal.Width)) + labs(subtitle = "Layer 2")
c <- b + geom_point() + labs(subtitle = "Layer 3")
d <- c + geom_smooth(method = "lm") + labs(subtitle = "Layer 4")
e <- d + labs(x = "Sepal Length (mm)", y = "Sepal Width (mm)") + labs(subtitle = "Layer 5")
f <- e + theme_minimal(base_family = "Times") + labs(subtitle = "Layer 6")

a + b + c + d + e + f + plot_layout(nrow = 2)
```

Importantly, each layer is independent and independently customisable. For example, the size, colour and position of each component can be adjusted, or one could, for example, remove the first geom to only visualise the line of best fit, simply by removing the layer that draws the data points (Figure \ \@ref(fig:remove-layer)). The use of layers makes it easy to build up complex plots step-by-step, and to adapt or extend plots from existing code.

```{r remove-layer, message=FALSE, echo=FALSE, fig.cap="Plot with scatterplot layer removed."}
iris %>% ggplot(aes(Sepal.Length, Sepal.Width)) + 
  geom_smooth(method = "lm") +
  labs(x = "Sepal Length (mm)", y = "Sepal Width (mm)")+
  theme_minimal()
```

## Simulated dataset

For the purpose of this tutorial, we will use simulated data for a 2 x 2 mixed-design Stroop test experiment. There are 100 rows (1 for each subject) and 7 variables:

- Participant information:
    -   `id`: Participant ID
    -   `age`: Age
- 1 between-subject IV:
    -   `language`:  Language group (1 = monolingual/2 = bilingual)
- 4 columns for the 2 dependent variables for RT and accuracy, crossed by the within-subject IV of condition:
    -   `rt`_cong`: Reaction time (ms) for congruent trials
    -   `rt`_incon`: Reaction time (ms) for incongruent trials
    -   `acc_cong`: Accuracy for congruent trials
    -   `acc_incon`: Accuracy for incongruent trials
    
The simulated dataset and tutorial code can be found in the online supplementary materials. For newcomers to R, we would suggest working through this tutorial with the simulated dataset, then extending the code to your own datasets with a similar structure, and finally generalising the code to new structures and problems. 

```{r data-sim, echo = FALSE}
# simulate data reproducibly
suppressPackageStartupMessages(library(faux))
faux_options(plot = FALSE)
set.seed(8675309)

wide <- sim_design(
  within= list(
    dv = c("rt", "acc"),
    type = c("cong", "incon")),
  between = list(language = c("1", "2")),
  n = list("1" = 55, "2" = 45),
  mu = data.frame(
    row.names = c("1", "2"),
    rt_cong = c(350, 360),
    rt_incon = c(600, 450),
    acc_cong = c(0, 0), # will convert to binomial later
    acc_incon = c(0, 0) # will convert to binomial later
  ),
  sd = c(50, 50, 1, 1, 50,  50, 1, 1),
      # RTi, a_c, a_i
  r = c( .5,  .6,  .3, # RTc
              .4,  .5, # RTi
                   .6) # a_c
) %>%
  mutate(
    acc_cong = norm2binom(acc_cong, size = 100, prob = .95),
    acc_incon = norm2binom(acc_incon, size = 100, prob = .85),
    # generate age with correlations to rt and none to acc
    age = rnorm_pre(data.frame(rt_cong, rt_incon, acc_cong, acc_incon), 
                    mu = 25, sd = 10, r = c(0.3, 0.3, 0, 0)),
    # truncate and round ages
    age = norm2trunc(age, 17.51, 60.49) %>% round()
  ) %>%
  select(id, age, everything())

# long <- pivot_longer(wide, 
#                      cols = rt_cong:acc_incon, 
#                      names_to = c("DV_type", "condition"), 
#                      names_sep = "_", 
#                      values_to = "DV")
# 
# dat <- pivot_wider(long, names_from = DV_type, values_from = DV)
write_csv(wide, "stroop_data.csv")
```

## Setting up R and RStudio

We strongly encourage the use of RStudio to write code in R. R is the programming language whilst RStudio is an *integrated development environment* that makes working with R easier. More information on installing both R and RStudio can be found in the additional resources.

Projects are a useful way of keeping all your code, data, and output in one place. To create a new project, open RStudio and click `File - New Project - New Directory - New Project`. You will be prompted to give the project a name, and select a location for where to store the project on your computer. Once you have done this, click `Create Project`. Download the simulated dataset from the online materials and save the file (  `stroop_data.csv`) to this folder. The files pane on the bottom right of RStudio should now display this folder and the files it contains - this is known as your *working directory* and it is where R will look for any data you wish to import and where it will save any output you create.

This tutorial will require you to use the packages contained with the `tidyverse` collection. Additionally, we will also require use of `patchwork`. To install these packages, copy and paste the below code into the console (the left hand pane) and press enter to execute the code.

```{r packages, eval = FALSE}
# only run in the console, never put this in a script 
package_list <- c("tidyverse", "patchwork")
install.packages(package_list)
```

Finally, so that you can save your code to return to later, open a new script `File - New File - R Script` and then save it using `File - Save`. R will default to saving the script in your project folder. This is where we will write all the tutorial code from now on. We have also provided an R Markdown copy of all below code in the supplementary materials. The reason that the install packages code is not included in the script is that every time you run the install command code it will install the latest version of the package and so leaving this code in your script can lead you to unintentionally install a package update you didn't want. For this reason, avoid including install code in any script. R scripts (or R code chunks if you are using Markdwon) treat anything you write in them as code by default. If you wish to write a comment with further information about your code (which we encourage you to do), you must use the hashtag sign to "comment it out".

```{r comments, eval = FALSE}
this_is_code
# this is a comment
```

## Preparing your data

Before you start visualizing your data, you need to get the data into an appropriate format. These preparatory steps can all be dealt with reproducibly using R and the additional resources section points to extra tutorials for doing so. However, performing these types of tasks in R can be difficult for new learners and the solutions and tools are dependent on the idiosyncrasies of each dataset. For this reason, in this tutorial we encourage the reader to complete these steps using the method they are most comfortable with and to focus on the aim of data visualisation.

### Data format

The simulated Stroop data is provided in a `csv` file rather than e.g., `xslx`. Functions exist in R to read many other types of data files, however, you can convert an `xlsx` spreadsheet to `csv` by using the `Save As` function in Microsoft Excel. Note that `csv` files strip all formatting and only store data in a single sheet; you may wish to create a `csv` file that only contains the data you wish to visualise that is part of a larger workbook. When working with your own data, any files you import should remove summary rows or additional notes and should only contains the rows and columns you want to plot.

### Variable names

Ensuring that your variable names are consistent can make it much easier to work in R. We recommend using short but informative variable names, for example `rt_cong` is preferred over `dv1_iv1` or `reaction_time_congruent_condition` because these are either hard to read or hard to type.

It is also helpful to have a consistent naming scheme, particularly for variable names that require more than one word. Two popular options are `CamelCase` where each new word begins with a capital letter, or `snake_case` where all letters are lower case and words are separated by an underscore. For the purposes of naming variables, avoid using any spaces in variable names (e.g., `rt cong`) and consider the additional meaning of a separator beyond making the variable names easier to read. For example, `rt_cong`, `rt_incon`, `acc_cong`, and `acc_incon` all have the DV to the left of the separator and the level of the IV to the right. `rt_cong_condition` on the other hand has two separators but only one of them is meaningful and it is useful to be able to split variable names consistently. In this paper, we will use `snake_case` and lower case letters for all variable names so that we don't have to remember where to put the capital letters.

When working with your own data, you can rename columns in Excel, but there are also instructions for how to do this in R in the additional resources.

### Data values

A great benefit to using R rather than SPSS is that categorical data can be entered as text. In the tutorial dataset, language group is entered as 1 or 2, so we can show you below how to recode numeric values into factors with labels, which also sets the order of display for graphs. However, we recommend recording meaningful labels rather than numbers to avoid misinterpreting data due to coding errors. Note that values must match *exactly* in order to be considered in the same category and R is case sensitive, so "mono", "Mono", and "monolingual" would be classified as members of three separate categories.

Finally, cells that represent missing data should be left empty rather than containing values like `NA`, `missing` or `0`. A complementary rule of thumb is that each column should only contain one type of data, such as words or numbers, not both.

